"""
Webhook Queue System
Ensures zero message loss under high load
"""
import asyncio
from datetime import datetime
from typing import Dict, Any
import json
import logging
from collections import deque

logger = logging.getLogger(__name__)


class WebhookQueue:
    """
    Reliable webhook queue with zero message loss
    Features:
    - Fast acknowledgment (< 100ms)
    - Background processing
    - Automatic retries
    - Dead letter queue for failed messages
    - Metrics tracking
    """
    
    def __init__(self, max_workers: int = 10, max_retries: int = 3):
        self.queue = asyncio.Queue(maxsize=10000)  # Large buffer
        self.processing_queue = deque()
        self.dead_letter_queue = deque(maxlen=1000)  # Failed messages
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.workers = []
        self.is_running = False
        
        # Metrics
        self.total_received = 0
        self.total_processed = 0
        self.total_failed = 0
        self.queue_size_peak = 0
        self.processing_times = deque(maxlen=100)
        
    async def start(self):
        """Start background workers"""
        if self.is_running:
            return
            
        self.is_running = True
        logger.info(f"Starting webhook queue with {self.max_workers} workers")
        
        # Start worker tasks
        for i in range(self.max_workers):
            worker = asyncio.create_task(self._worker(i))
            self.workers.append(worker)
            
        logger.info(f"‚úÖ {self.max_workers} workers started and ready")
    
    async def stop(self):
        """Gracefully stop all workers"""
        logger.info("Stopping webhook queue...")
        self.is_running = False
        
        # Wait for current processing to complete
        await self.queue.join()
        
        # Cancel workers
        for worker in self.workers:
            worker.cancel()
            
        await asyncio.gather(*self.workers, return_exceptions=True)
        logger.info("‚úÖ Webhook queue stopped")
    
    async def enqueue(self, webhook_data: Dict[str, Any]) -> bool:
        """
        Add webhook to queue
        Returns immediately (fast acknowledgment)
        """
        try:
            # Add metadata
            message = {
                "data": webhook_data,
                "received_at": datetime.utcnow().isoformat(),
                "retry_count": 0,
                "message_id": f"{datetime.utcnow().timestamp()}_{self.total_received}"
            }
            
            # Non-blocking put (with timeout)
            await asyncio.wait_for(
                self.queue.put(message),
                timeout=1.0
            )
            
            self.total_received += 1
            
            # Update peak queue size
            current_size = self.queue.qsize()
            if current_size > self.queue_size_peak:
                self.queue_size_peak = current_size
            
            logger.debug(f"üì• Webhook enqueued (queue size: {current_size})")
            return True
            
        except asyncio.TimeoutError:
            logger.error("‚ö†Ô∏è Queue full! Message may be lost. Consider increasing queue size.")
            self.total_failed += 1
            return False
        except Exception as e:
            logger.error(f"‚ùå Error enqueueing webhook: {e}")
            self.total_failed += 1
            return False
    
    async def _worker(self, worker_id: int):
        """Background worker that processes webhooks"""
        logger.info(f"Worker {worker_id} started")
        
        while self.is_running:
            try:
                # Get message from queue
                message = await asyncio.wait_for(
                    self.queue.get(),
                    timeout=1.0
                )
                
                start_time = datetime.utcnow()
                
                try:
                    # Process the webhook
                    await self._process_webhook(message)
                    
                    # Track processing time
                    processing_time = (datetime.utcnow() - start_time).total_seconds()
                    self.processing_times.append(processing_time)
                    
                    self.total_processed += 1
                    logger.debug(f"‚úÖ Worker {worker_id} processed message (took {processing_time:.2f}s)")
                    
                except Exception as e:
                    logger.error(f"‚ùå Worker {worker_id} error processing message: {e}")
                    
                    # Retry logic
                    message["retry_count"] += 1
                    
                    if message["retry_count"] <= self.max_retries:
                        logger.info(f"üîÑ Retrying message (attempt {message['retry_count']}/{self.max_retries})")
                        await self.queue.put(message)  # Re-queue for retry
                    else:
                        logger.error(f"üíÄ Message failed after {self.max_retries} retries - moving to dead letter queue")
                        self.dead_letter_queue.append({
                            **message,
                            "failed_at": datetime.utcnow().isoformat(),
                            "error": str(e)
                        })
                        self.total_failed += 1
                
                finally:
                    self.queue.task_done()
                    
            except asyncio.TimeoutError:
                # No messages in queue, continue
                continue
            except asyncio.CancelledError:
                logger.info(f"Worker {worker_id} cancelled")
                break
            except Exception as e:
                logger.error(f"Worker {worker_id} unexpected error: {e}")
                await asyncio.sleep(1)  # Back off on error
    
    async def _process_webhook(self, message: Dict[str, Any]):
        """
        Process a single webhook message
        This is where we parse and store in database
        """
        from database.database import get_db
        from database.schemas import WebhookPayload
        from services.order_processor import OrderProcessor
        from services.webhook_parser import parse_webhook_content
        
        webhook_data = message["data"]
        
        # Parse webhook content
        body_text = webhook_data.get("body", "")
        content_type = webhook_data.get("content_type", "")
        
        payload_dict = parse_webhook_content(body_text, content_type)
        
        # Validate
        webhook_payload = WebhookPayload(**payload_dict)
        
        # Store in database
        async with get_db() as db:
            processor = OrderProcessor(db)
            order = await processor.create_order_from_webhook(webhook_payload)
            
            logger.info(f"‚úÖ Processed order: {order.order_number} (ID: {order.id})")
            
            # Broadcast via WebSocket
            try:
                from main import manager
                await manager.broadcast({
                    "type": "new_order",
                    "order": {
                        "id": order.id,
                        "order_number": order.order_number,
                        "product": order.product,
                        "price": order.price,
                        "status": order.status.value,
                        "created_at": order.created_at.isoformat(),
                    }
                })
            except Exception as e:
                logger.warning(f"WebSocket broadcast failed: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get queue statistics"""
        avg_processing_time = (
            sum(self.processing_times) / len(self.processing_times)
            if self.processing_times else 0
        )
        
        return {
            "queue_size": self.queue.qsize(),
            "queue_size_peak": self.queue_size_peak,
            "total_received": self.total_received,
            "total_processed": self.total_processed,
            "total_failed": self.total_failed,
            "dead_letter_queue_size": len(self.dead_letter_queue),
            "workers_running": len([w for w in self.workers if not w.done()]),
            "workers_total": self.max_workers,
            "avg_processing_time_ms": round(avg_processing_time * 1000, 2),
            "is_running": self.is_running,
            "success_rate": round(
                (self.total_processed / self.total_received * 100)
                if self.total_received > 0 else 100,
                2
            )
        }
    
    async def get_dead_letters(self, limit: int = 100) -> list:
        """Get failed messages for manual review"""
        return list(self.dead_letter_queue)[-limit:]
    
    async def retry_dead_letters(self) -> int:
        """Retry all messages in dead letter queue"""
        retried = 0
        
        while self.dead_letter_queue:
            message = self.dead_letter_queue.popleft()
            message["retry_count"] = 0  # Reset retry count
            await self.queue.put(message)
            retried += 1
        
        logger.info(f"‚ôªÔ∏è Retrying {retried} messages from dead letter queue")
        return retried


# Global queue instance
webhook_queue = WebhookQueue(max_workers=10, max_retries=3)

